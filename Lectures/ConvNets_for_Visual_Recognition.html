<!DOCTYPE html>
<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>CS231n Convolutional Neural Networks for Visual Recognition</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Course materials and notes for Stanford class CS231n: Convolutional Neural Networks for Visual Recognition.">
    <link rel="canonical" href="http://cs231n.github.io/understanding-cnn/">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="ConvNets_for_Visual_Recognition_files/main.css">

    <!-- Google fonts -->
    <link href="ConvNets_for_Visual_Recognition_files/css.css" rel="stylesheet" type="text/css">

    <!-- Google tracking -->
    <script src="ConvNets_for_Visual_Recognition_files/analytics.js" async=""></script><script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-46895817-2', 'auto');
      ga('send', 'pageview');

    </script>
    
<style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Hover_Arrow {position: absolute; width: 15px; height: 11px; cursor: pointer}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; color: #666666}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_Menu_Close {position: absolute; width: 31px; height: 31px; top: -15px; left: -15px}
</style><style type="text/css">.MathJax_Preview .MJXc-math {color: inherit!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1px; bottom: 2px; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXc-script {font-size: .8em}
.MJXc-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXc-bold {font-weight: bold}
.MJXc-italic {font-style: italic}
.MJXc-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXc-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXc-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXc-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXc-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXc-largeop {font-size: 150%}
.MJXc-largeop.MJXc-int {vertical-align: -.2em}
.MJXc-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXc-display {display: block; text-align: center; margin: 1em 0}
.MJXc-math span {display: inline-block}
.MJXc-box {display: block!important; text-align: center}
.MJXc-box:after {content: " "}
.MJXc-rule {display: block!important; margin-top: .1em}
.MJXc-char {display: block!important}
.MJXc-mo {margin: 0 .15em}
.MJXc-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXc-denom {display: inline-table!important; width: 100%}
.MJXc-denom > * {display: table-row!important}
.MJXc-surd {vertical-align: top}
.MJXc-surd > * {display: block!important}
.MJXc-script-box > *  {display: table!important; height: 50%}
.MJXc-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXc-script-box > *:last-child > * {vertical-align: bottom}
.MJXc-script-box > * > * > * {display: block!important}
.MJXc-mphantom {visibility: hidden}
.MJXc-munderover {display: inline-table!important}
.MJXc-over {display: inline-block!important; text-align: center}
.MJXc-over > * {display: block!important}
.MJXc-munderover > * {display: table-row!important}
.MJXc-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXc-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXc-mtr {display: table-row!important}
.MJXc-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXc-mtr > .MJXc-mtd:first-child {padding-left: 0}
.MJXc-mtr:first-child > .MJXc-mtd {padding-top: 0}
.MJXc-mlabeledtr {display: table-row!important}
.MJXc-mlabeledtr > .MJXc-mtd:first-child {padding-left: 0}
.MJXc-mlabeledtr:first-child > .MJXc-mtd {padding-top: 0}
.MJXc-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXc-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXc-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXc-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXc-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXc-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXc-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXc-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXc-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXc-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXc-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_CHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head>


    <body><div style="display: none;" id="MathJax_Message"></div>

    <header class="site-header">

  <div class="wrap title-wrap">
    <a class="site-title" href="http://cs231n.github.io/">CS231n Convolutional Neural Networks for Visual Recognition</a>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1></h1>
  </header>

  <article class="post-content">
  <p><a name="vis"></a></p>

<p>(this page is currently in draft form)</p>

<h2>Visualizing what ConvNets learn</h2>

<p>Several approaches for understanding and visualizing Convolutional 
Networks have been developed in the literature, partly as a response the
 common criticism that the learned features in a Neural Network are not 
interpretable. In this section we briefly survey some of these 
approaches and related work.</p>

<h3>Visualizing the activations and first-layer weights</h3>

<p><strong>Layer Activations</strong>. The most straight-forward 
visualization technique is to show the activations of the network during
 the forward pass. For ReLU networks, the activations usually start out 
looking relatively blobby and dense, but as the training progresses the 
activations usually become more sparse and localized. One dangerous 
pitfall that can be easily noticed with this visualization is that some 
activation maps may be all zero for many different inputs, which can 
indicate <em>dead</em> filters, and can be a symptom of high learning rates.</p>

<div class="fig figcenter fighighlight">
  <img src="ConvNets_for_Visual_Recognition_files/act1.jpg" width="49%">
  <img src="ConvNets_for_Visual_Recognition_files/act2.jpg" width="49%">
  <div class="figcaption">
    Typical-looking activations on the first CONV layer (left), and the 
5th CONV layer (right) of a trained AlexNet looking at a picture of a 
cat. Every box shows an activation map corresponding to some filter. 
Notice that the activations are sparse (most values are zero, in this 
visualization shown in black) and mostly local.
  </div>
</div>

<p><strong>Conv/FC Filters.</strong> The second common strategy is to 
visualize the weights. These are usually most interpretable on the first
 CONV layer which is looking directly at the raw pixel data, but it is 
possible to also show the filter weights deeper in the network. The 
weights are useful to visualize because well-trained networks usually 
display nice and smooth filters without any noisy patterns. Noisy 
patterns can be an indicator of a network that hasn't been trained for 
long enough, or possibly a very low regularization strength that may 
have led to overfitting.</p>

<div class="fig figcenter fighighlight">
  <img src="ConvNets_for_Visual_Recognition_files/filt1.jpg" width="49%">
  <img src="ConvNets_for_Visual_Recognition_files/filt2.jpg" width="49%">
  <div class="figcaption">
    Typical-looking filters on the first CONV layer (left), and the 2nd 
CONV layer (right) of a trained AlexNet. Notice that the first-layer 
weights are very nice and smooth, indicating nicely converged network. 
The color/grayscale features are clustered because the AlexNet contains 
two separate streams of processing, and an apparent consequence of this 
architecture is that one stream develops high-frequency grayscale 
features and the other low-frequency color features. The 2nd CONV layer 
weights are not as interpretible, but it is apparent that they are still
 smooth, well-formed, and absent of noisy patterns.
  </div>
</div>

<h3>Retrieving images that maximally activate a neuron</h3>

<p>Another visualization technique is to take a large dataset of images,
 feed them through the network and keep track of which images maximally 
activate some neuron. We can then visualize the images to get an 
understanding of what the neuron is looking for in its receptive field. 
One such visualization (among others) is shown in <a href="http://arxiv.org/abs/1311.2524">Rich feature hierarchies for accurate object detection and semantic segmentation</a> by Ross Girshick et al.:</p>

<div class="fig figcenter fighighlight">
  <img src="ConvNets_for_Visual_Recognition_files/pool5max.jpg" width="100%">
  <div class="figcaption">
    Maximally activating images for some POOL5 (5th pool layer) neurons 
of an AlexNet. The activation values and the receptive field of the 
particular neuron are shown in white. (In particular, note that the 
POOL5 neurons are a function of a relatively large portion of the input 
image!) It can be seen that some neurons are responsive to upper bodies,
 text, or specular highlights.
  </div>
</div>

<p>One problem with this approach is that ReLU neurons do not 
necessarily have any semantic meaning by themselves. Rather, it is more 
appropriate to think of multiple ReLU neurons as the basis vectors of 
some space that represents in image patches. In other words, the 
visualization is showing the patches at the edge of the cloud of 
representations, along the (arbitrary) axes that correspond to the 
filter weights. This can also be seen by the fact that neurons in a 
ConvNet operate linearly over the input space, so any arbitrary rotation
 of that space is a no-op. This point was further argued in <a href="http://arxiv.org/abs/1312.6199">Intriguing properties of neural networks</a> by Szegedy et al., where they perform a similar visualization along aribtrary directions in the representation space.</p>

<h3>Embedding the codes with t-SNE</h3>

<p>ConvNets can be interpreted as gradually transforming the images into
 a representation in which the classes are separable by a linear 
classifier. We can get a rough idea about the topology of this space by 
embedding images into two dimensions so that their low-dimensional 
representation has approximately equal distances than their 
high-dimensional representation. There are many embedding methods that 
have been developed with the intuition of embedding high-dimensional 
vectors in a low-dimensional space while preserving the pairwise 
distances of the points. Among these, <a href="http://lvdmaaten.github.io/tsne/">t-SNE</a> is one of the best-known methods that consistently produces visually-pleasing results.</p>

<p>To produce an embedding, we can take a set of images and use the 
ConvNet to extract the CNN codes (e.g. in AlexNet the 4096-dimensional 
vector right before the classifier, and crucially, including the ReLU 
non-linearity). We can then plug these into t-SNE and get 2-dimensional 
vector for each image. The corresponding images can them be visualized 
in a grid:</p>

<div class="fig figcenter fighighlight">
  <img src="ConvNets_for_Visual_Recognition_files/tsne.jpg" width="100%">
  <div class="figcaption">
    t-SNE embedding of a set of images based on their CNN codes. Images 
that are nearby each other are also close in the CNN representation 
space, which implies that the CNN "sees" them as being very similar. 
Notice that the similarities are more often class-based and semantic 
rather than pixel and color-based. For more details on how this 
visualization was produced the associated code, and more related 
visualizations at different scales refer to <a href="http://cs.stanford.edu/people/karpathy/cnnembed/">t-SNE visualization of CNN codes</a>.
  </div>
</div>

<h3>Occluding parts of the image</h3>

<p>Suppose that a ConvNet classifies an image as a dog. How can we be 
certain that it's actually picking up on the dog in the image as opposed
 to some contextual cues from the background or some other miscellaneous
 object? One way of investigating which part of the image some 
classification prediction is coming from is by plotting the probability 
of the class of interest (e.g. dog class) as a function of the position 
of an occluder object. That is, we iterate over regions of the image, 
set a patch of the image to be all zero, and look at the probability of 
the class. We can visualize the probability as a 2-dimensional heat map.
 This approach has been used in Matthew Zeiler's <a href="http://arxiv.org/abs/1311.2901">Visualizing and Understanding Convolutional Networks</a>:</p>

<div class="fig figcenter fighighlight">
  <img src="ConvNets_for_Visual_Recognition_files/occlude.jpg" width="100%">
  <div class="figcaption">
    Three input images (top). Notice that the occluder region is shown 
in grey. As we slide the occluder over the image we record the 
probability of the correct class and then visualize it as a heatmap 
(shown below each image). For instance, in the left-most image we see 
that the probability of Pomeranian plummets when the occluder covers the
 face of the dog, giving us some level of confidence that the dog's face
 is primarily responsible for the high classification score. Conversely,
 zeroing out other parts of the image is seen to have relatively 
negligible impact.
  </div>
</div>

<h3>Visualizing the data gradient and friends</h3>

<p><strong>Data Gradient</strong>.</p>

<p><a href="http://arxiv.org/abs/1312.6034">Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</a></p>

<p><strong>DeconvNet</strong>.</p>

<p><a href="http://arxiv.org/abs/1311.2901">Visualizing and Understanding Convolutional Networks</a></p>

<p><strong>Guided Backpropagation</strong>.</p>

<p><a href="http://arxiv.org/abs/1412.6806">Striving for Simplicity: The All Convolutional Net</a></p>

<h3>Reconstructing original images based on CNN Codes</h3>

<p><a href="http://arxiv.org/abs/1412.0035">Understanding Deep Image Representations by Inverting Them</a></p>

<h3>How much spatial information is preserved?</h3>

<p><a href="http://papers.nips.cc/paper/5420-do-convnets-learn-correspondence.pdf">Do ConvNets Learn Correspondence?</a> (tldr: yes)</p>

<h3>Plotting performance as a function of image attributes</h3>

<p><a href="http://arxiv.org/abs/1409.0575">ImageNet Large Scale Visual Recognition Challenge</a></p>

<h2>Fooling ConvNets</h2>

<p><a href="http://arxiv.org/abs/1412.6572">Explaining and Harnessing Adversarial Examples</a></p>

<h2>Comparing ConvNets to Human labelers</h2>

<p><a href="http://karpathy.github.io/2014/09/02/what-i-learned-from-competing-against-a-convnet-on-imagenet/">What I learned from competing against a ConvNet on ImageNet</a></p>

  </article>

</div>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <div class="footer-col-1 column">
      <ul>
        
        <li>
          <a href="https://github.com/cs231n">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"></path>
              </svg>
            </span>
            <span class="username">cs231n</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/cs231n">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"></path>
              </svg>
            </span>
            <span class="username">cs231n</span>
          </a>
        </li>
        <li>
          <a href="mailto:karpathy@cs.stanford.edu">karpathy@cs.stanford.edu</a>
        </li>
      </ul>
    </div>

    <div class="footer-col-2 column">
        
    </div>

    <div class="footer-col-3 column">
      
    </div>

  </div>

</footer>


    <!-- mathjax -->
    <script type="text/javascript" src="ConvNets_for_Visual_Recognition_files/MathJax.js"></script>
    
</body></html>